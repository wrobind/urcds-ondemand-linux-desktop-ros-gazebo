#!/usr/bin/env bash

# Change working directory to user's home directory
cd "${HOME}"

set -x

# Reset module environment (may require login shell for some HPC clusters)
module purge && module restore

export SINGULARITYENV_XDG_CONFIG_HOME=$XDG_CONFIG_HOME

container_image=/n/singularity_images/Academic-cluster/<%= context.semester %>/<%= context.course %>/<%= context.centos_version %>

export SING_BINDS=" $SING_BINDS  -B /etc/nsswitch.conf -B /etc/sssd/ -B /var/lib/sss -B /etc/slurm -B /slurm -B /var/run/munge  -B `which sbatch ` -B `which srun ` -B `which sacct ` -B `which scontrol `   -B /usr/lib64/slurm/  -B /usr/lib64/libmunge.so.2 -B /usr/lib64/libmunge.so.2.0.0 "

export SING_GPU=""

<%- if !context.custom_num_gpus.to_i.zero? -%>
export SING_GPU="--nv"
<%- end -%>

# Start up desktop
echo "Launching desktop '<%= context.desktop %>'..."
chmod +x "<%= session.staged_root.join("desktops", "#{context.desktop}.sh") %>"
export SINGULARITYENV_OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
singularity exec $SING_GPU  $SING_BINDS $SING_OVERLAYS $SING_BINDS $container_image "<%= session.staged_root.join("desktops", "#{context.desktop}.sh") %>"
echo "Desktop '<%= context.desktop %>' ended..."
